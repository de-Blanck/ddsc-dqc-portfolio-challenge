---
phase: 01-data-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/evaluate.py
  - instance.json
autonomous: true

must_haves:
  truths:
    - "instance.json exists with mu vector, Sigma matrix, and locked parameters (N=20, K=5, lambda=0.5, A=10.0)"
    - "Data covers date range 2023-01-01 to 2025-12-31 for all 20 tickers"
    - "Covariance matrix is positive semi-definite"
    - "JSON serialization is deterministic (sort_keys=True) so hash is reproducible"
    - "At least 200 aligned return observations used for covariance estimation"
  artifacts:
    - path: "instance.json"
      provides: "Frozen QUBO instance with mu, sigma, tickers, parameters"
      contains: "\"K\": 5"
    - path: "scripts/evaluate.py"
      provides: "Instance builder with deterministic JSON output"
      contains: "sort_keys=True"
  key_links:
    - from: "scripts/evaluate.py"
      to: "instance.json"
      via: "write_instance_json with sort_keys=True"
      pattern: "sort_keys=True"
    - from: "scripts/download_stooq.py"
      to: "data/prices/*.csv"
      via: "HTTP download from Stooq"
      pattern: "stooq.com"
---

<objective>
Download Stooq price data for all 20 tickers, fix evaluate.py to produce deterministic JSON output, validate the data quality inline, and generate the frozen instance.json at the repo root.

Purpose: This is the foundational artifact that the entire challenge depends on. Participants need instance.json to solve the QUBO. It must be validated, deterministic, and committed.
Output: instance.json in repo root with mu, Sigma, and locked parameters. evaluate.py updated with sort_keys=True for deterministic hashing.
</objective>

<execution_context>
@C:\Users\Rune\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Rune\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-foundation/01-RESEARCH.md
@scripts/evaluate.py
@scripts/download_stooq.py
@data/tickers.txt
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix evaluate.py for deterministic JSON and download + generate instance</name>
  <files>scripts/evaluate.py, instance.json</files>
  <action>
1. Modify `write_instance_json()` in scripts/evaluate.py to add `sort_keys=True` to the `json.dump()` call. The current line is:
   ```python
   json.dump(payload, f, indent=2)
   ```
   Change to:
   ```python
   json.dump(payload, f, indent=2, sort_keys=True, ensure_ascii=False)
   ```
   Also add a trailing newline after the JSON dump (`f.write('\n')`) for POSIX compliance.

2. Run the data download:
   ```bash
   python scripts/download_stooq.py
   ```
   This downloads 20 CSV files to data/prices/. Expect it to take ~10 seconds (0.5s sleep between requests).

3. After download completes, verify all 20 CSVs exist in data/prices/ and each has data rows (not empty or error responses). Stooq sometimes returns HTML error pages instead of CSV. Check the first line of each file contains column headers (Date, Open, High, Low, Close, Volume or similar).

4. Generate instance.json at the REPO ROOT (not data/instance.json — the roadmap says "repo root"):
   ```bash
   python scripts/evaluate.py --K 5 --lambda_ 0.5 --penalty_A 10.0 --start 2023-01-01 --end 2025-12-31 --emit_instance instance.json
   ```
   Note: Use `--emit_instance instance.json` to place it at repo root as specified in the roadmap success criteria.

5. After generation, perform inline validation by running a quick Python check:
   - Load instance.json and verify it has keys: tickers, K, lambda, penalty_A, mu, sigma
   - Verify len(tickers) == 20, len(mu) == 20, len(sigma) == 20, all sigma rows have length 20
   - Verify K==5, lambda==0.5, penalty_A==10.0
   - Verify covariance matrix is symmetric: sigma[i][j] == sigma[j][i] for all i,j
   - Verify positive semi-definite: all eigenvalues of sigma >= -1e-10 (use numpy.linalg.eigvalsh)
   - Print the number of aligned observations (this comes from the evaluate.py output — it requires >= 200)
   - Print min and max eigenvalues of sigma

6. Verify deterministic output: Run the generation command again with a different output path (e.g., /tmp/instance_check.json), then compare SHA256 hashes of both files. They MUST match. If they don't, investigate non-determinism.

IMPORTANT: If Stooq download fails for any ticker (returns HTML error page or empty file), retry that ticker individually after a 2-second delay. If it still fails, report the ticker and stop — do NOT generate instance.json from incomplete data.

IMPORTANT: If the evaluate.py script reports fewer than 200 aligned return rows, stop and report — the date range may need adjusting.
  </action>
  <verify>
- `python -c "import json; d=json.load(open('instance.json')); assert len(d['tickers'])==20; assert d['K']==5; assert d['lambda']==0.5; assert d['penalty_A']==10.0; print('Schema OK')"` prints "Schema OK"
- `python -c "import json,numpy as np; d=json.load(open('instance.json')); S=np.array(d['sigma']); e=np.linalg.eigvalsh(S); assert e.min()>-1e-10; print(f'PSD OK, min eigenvalue={e.min():.2e}')"` shows positive min eigenvalue
- `python -c "import json; f=open('instance.json'); content=f.read(); assert '\"sort_keys\"' not in content; import hashlib; h=hashlib.sha256(content.encode()).hexdigest(); print(f'SHA256: {h}')"` prints a SHA256 hash
- `grep 'sort_keys=True' scripts/evaluate.py` returns a match
  </verify>
  <done>
- instance.json exists at repo root with all 20 tickers, correct parameters (K=5, lambda=0.5, penalty_A=10.0), 20-element mu vector, 20x20 sigma matrix
- Sigma is symmetric and positive semi-definite (all eigenvalues >= -1e-10)
- evaluate.py uses sort_keys=True for deterministic JSON
- SHA256 hash is reproducible (generating twice produces identical output)
  </done>
</task>

<task type="auto">
  <name>Task 2: Validate price data quality and spot-check for Stooq issues</name>
  <files>data/prices/*.csv</files>
  <action>
Run a Python validation script inline (no need to create a separate file — this is a one-time check) that:

1. For each of the 20 tickers in data/prices/:
   a. Load the CSV using the same load_stooq_csv() from evaluate.py
   b. Filter to 2023-01-01 through 2025-12-31
   c. Check date coverage: first date <= 2023-01-02 (first trading day), last date >= 2025-12-30 (or close to 2025-12-31)
   d. Count trading days — expect roughly 750 (3 years * ~252 trading days). Flag if < 700.
   e. Compute daily log returns and check for extreme values:
      - Flag any daily return > 20% absolute as suspicious
      - Count total extreme returns per ticker
      - If any ticker has > 1% of its returns as extreme, report it
   f. Check for gaps: find the maximum gap between consecutive dates. Flag if > 5 business days (suggests missing data).

2. Print a summary table:
   ```
   Ticker | Start Date | End Date | Trading Days | Extreme Returns | Max Gap (days)
   ```

3. If any ticker fails critical checks (date coverage insufficient, < 500 trading days), stop and report. These are warnings-only for extreme returns and gaps — Stooq data quality issues are expected and documented.

IMPORTANT: This is a validation-only task. Do NOT modify any files. The purpose is to verify data quality before we commit instance.json. If critical issues are found, report them so the user can decide how to proceed.
  </action>
  <verify>
- The validation script runs without errors
- All 20 tickers have data covering 2023-01-01 to 2025-12-31
- All 20 tickers have >= 500 trading days in range
- Summary table is printed showing all 20 tickers passed
  </verify>
  <done>
- All 20 tickers validated for date coverage (2023-01-01 to 2025-12-31)
- No critical data quality issues (missing tickers, insufficient data)
- Any Stooq data quality warnings documented in execution output
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. instance.json exists at repo root with correct schema and parameters
2. evaluate.py has deterministic JSON output (sort_keys=True)
3. All 20 tickers have validated price data covering the required date range
4. Covariance matrix is positive semi-definite
5. SHA256 hash is reproducible
</verification>

<success_criteria>
- instance.json at repo root passes schema validation (20 tickers, K=5, lambda=0.5, A=10.0, 20x20 sigma)
- Sigma matrix is positive semi-definite
- evaluate.py write_instance_json uses sort_keys=True
- All 20 price CSVs cover 2023-01-01 to 2025-12-31 with >= 500 trading days each
- Generating instance.json twice produces identical SHA256 hash
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-01-SUMMARY.md`
</output>
